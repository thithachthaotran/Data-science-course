{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52072ae",
   "metadata": {},
   "source": [
    "### Name: Thi Thach Thao Tran\n",
    "Student ID: 47764554"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753bbb2",
   "metadata": {},
   "source": [
    "### The process of solving problems and learning to use notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fbe57",
   "metadata": {},
   "source": [
    "According to the Cross-Industry Standard Process for Data Mining (CRISP-DM) model, there are six main phases of a data analytics project including Business Understanding, Data Understanding, Data preparation, Modelling, Model Evaluation and Deployment. After the work in the past four portfolios, I had a glimpse of these workflows in a project, especially portfolio 4 where I encountered the whole process with a topic of my interest. In the first three portfolios, the attention is on the “How”, for example how to write the code correctly with proper logic and technical steps. Moving on to this fourth portfolio, my mindset was trained to shift from “How” to “Why”. For instance, why I chose this data, why I chose this model and what is the suitable baseline model. The challenges also lie in this “Why” because each technique will be used to solve a certain problem, and that is unique to each dataset. It was much easier to remove the number of patients under 18 years of age than deciding which level of ages was outliers to remove.\n",
    "\n",
    "In terms of platform, Jupyter notebook is helpful for my coding experience. The practical class played an important role in framing Python for me and giving instruction using notebooks. In my personal experience, I find notebooks have some benefits that can outweigh other code editors. Firstly, the interactive nature of notebooks make them easy to experiment, easy to code step by step. After that, we can see the results of the code immediately. Hence, it was easy to debug or spot the mistake line by line. Lastly, notebooks provide wonderful presentations with markdown, code and graphs, so that people can present analysis mixing between code, text, data and graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314050b3",
   "metadata": {},
   "source": [
    "### My progress from the start of the unit and my interest in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34e57a",
   "metadata": {},
   "source": [
    "I made huge progress compared to the first day of the course. Speaking of knowledge and skills development, I have acquired a deeper understanding of data science concepts, techniques, and tools. Python is now one of my new programming languages that I can use proficiently to proceed with statistical analysis, machine learning algorithms, data visualization, and data manipulation.\n",
    "\n",
    "In addition, I had the opportunity to engage in practical projects in the four previous portfolios. This involved working on datasets, solving problems and applying different machine learning models to make predictions. These practical experiences have contributed to enhancing my analytical skills and revising the theoretical knowledge gained along the unit.\n",
    "\n",
    "Going towards the end of this unit, I understand that there are plenty of specific areas of specialization I can pursue in the future. Some people may find their interests in machine learning and predictive modeling, but I lean towards data visualization. The knowledge from this course has laid a foundation for my understanding, so now I have various possibilities to choose and study higher level subjects of data visualization and analytics techniques. Continuous learning and skill development are unquestionably necessary for staying up-to-date in this continuously expanding sector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032dba3",
   "metadata": {},
   "source": [
    "### Discussion points based on portfolio 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0905546",
   "metadata": {},
   "source": [
    "#### The reason I chose the dataset I have used for my portfolio:\n",
    "\n",
    "Stroke is the second-leading cause of death and the primary global cause of disability. According to the Global Stroke Factsheet published in 2022, the lifetime risk of having a stroke has climbed by 50% in the last 17 years, with 1 in 4 people now thought to experience one. Doctors and researchers constantly invest time and money to find a way to prevent or give a warning to people who have high chances of stroke. At the same time, predicting the probability of a person having certain diseases is one useful implementation of Machine Learning. That was the reason why I framed my area of interest was health care in general, and stroke, in specific. \n",
    "\n",
    "Furthermore, this dataset contains features that provide enough information to carry out the analysis and prediction. It has 5110 records in total with 11 features. The `stroke` column is the dependent variable and other factors that could be related to the results included `gender`, `age`, various diseases, and `smoking status`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5554637",
   "metadata": {},
   "source": [
    "#### The reason to choose my machine models and why these models are suitable for solving the problem I have raised:\n",
    "\n",
    "The machine models that I have chosen in portfolio 4 were Logistic Regression Model and K-nearest neighbors (KNN) Model.  \n",
    "\n",
    "- **Logistic Regression Model:** Logistic regression is a statistical modeling technique used to predict binary or categorical outcomes. It is particularly useful when I have a dependent variable that is binary or dichotomous and I want to understand the relationship between predictor variables and the probability of the outcome occurring. My dataset is aimed at predicting whether a person will had stroke or not based on their health status and demographic information. This is a common binary classification problem (with 1 means had a stroke and 0 is not). Hence, logistic regression is a suitable model to establish a baseline classification performance.\n",
    "\n",
    "- **KNN Model:** The k-nearest neighbors (KNN) algorithm is a flexible machine learning technique used for both classification and regression tasks. For noisy data, KNN can be robust to noisy or imperfect data. Compared to other methods that try to fit a global model, it may be less affected by outliers or noise in the dataset because it depends on local patterns and nearest neighbours. I also used GridSearchCV to improve the model by selecting the most suitable number of K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438049f7",
   "metadata": {},
   "source": [
    "In conclusion, although there is plenty room for improvements that I could make in the portfolio, I believe I learnt a lot from the process and the challenges encountered when solving my problems. I am grateful for the opportunity taking this course, learnt how to think right and how to do right.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec677a17",
   "metadata": {},
   "source": [
    "### References:\n",
    "Global Stroke Factsheet. International Journal of Stroke. 2022, Vol. 17(1) 18–29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
